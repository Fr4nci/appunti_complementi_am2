\chapter{Spazi euclidei}
	\pagestyle{plain}
	\thispagestyle{empty}
	\pagestyle{fancy}	
	In questo capitolo, ci soffermeremo su alcuni risultati che possono essere ottenuti considerando uno spazio euclideo e sullo studio della sua topologia, siccome molti concetti dell'Analisi 1 devono essere concettualmente rivisti per poter essere generalizzati a spazi di dimensione diversa da 1. \\
	Nell'utilizzo a dimensioni maggiore di $1$ siamo quasi "obbligati" a fare riferimento ad alcuni strumenti presi dal corso di Geometria per sviluppare correttamente la teoria, pertanto per una trattazione più accurata rimando ai libri più celebri riguardo a Geometria come \cite{lang} oppure \cite{sernesi}.
	
	\section{Spazio euclideo e prodotto scalare}
	Partiamo ricordando al lettore la definizione di prodotto scalare/hermitiano e di \textbf{spazio euclideo}:
	\begin{definition}[prodotto scalare/hermitiano]
	dato $V$ spazio vettoriale sul campo $\mathbb{R}$ ($\mathbb{C}$) si definisce prodotto scalare (hermitiano) un'applicazione $\varphi: V \times V \to \mathbb{R}$ ($\varphi: V \times V \to \mathbb{C}$) che gode delle seguenti proprietà:
	\begin{enumerate}[label=\protect\circled{\arabic*}]
		\item lineare nella prima componente, ovvero
		$$
		\forall \underline{v}, \underline{w}, \underline{z}, \forall \alpha, \beta \in \mathbb{R}, \varphi(\alpha \underline{v} + \beta \underline{w}, \underline{z}) = \alpha \varphi(\underline{v}, \underline{z}) + \beta \varphi(\underline{w}, \underline{z})
		$$
		\item lineare nella seconda componente, ovvero
		$$
		\forall \underline{v}, \underline{w}, \underline{z}, \forall \alpha \beta \in \mathbb{R}, \varphi(\underline{v}, \alpha \underline{w} + \beta \underline{z}) = \alpha \varphi(\underline{v}, \underline{w}) + \beta \varphi(\underline{v}, \underline{z})
		$$
		\item simmetrica, ovvero
		$$
		\forall \underline{v}, \underline{w} \in V \, \, , \varphi(\underline{v}, \underline{w}) = \varphi(\underline{w}, \underline{v})
		$$
	\end{enumerate}
	(sostituendo $\mathbb{C}$ al posto di $\mathbb{R}$ si ottengono le proprietà che rendono identificano un prodotto hermitiano)
	\end{definition}
	\begin{definition}[spazio euclideo]
		uno spazio vettoriale $V$ munito di un prodotto scalare $\varphi: V \times V \to \mathbb{R}$ (analogamente nel caso di un prodotto hermitiano) definito positivo ($\forall \underline{v} \neq \underline{0}, \varphi(\underline{v}, \underline{v}) > 0$) si dice \textbf{spazio euclideo}
	\end{definition}
	\begin{remark}
	Quando un prodotto scalare/hermitiano è definito positivo, diciamo che è \textbf{coercivo}. Durante questo documento capiterà spesso di riferirci a questa proprietà con questo termine
	\end{remark}
	\noindent Introduciamo qualche notazione a noi utile:
	\begin{itemize}
		\item $E_n$: spazio affine euclideo di dimensione finita $n$;
		\item $\mathbb{E}_n$: spazio vettoriale euclideo, ottenuto fissando un'origine in $E_n$;
	\end{itemize}
	Sappiamo, dal corso di Geometria, che, fissando una base $\mathcal{B}$ di $\mathbb{E}_n$, le coordinate di un generico vettore $\underline{x} \in \mathbb{E}_n$ sono univoche: possiamo definire l'applicazione lineare $\varphi: \mathbb{E}_n \to \mathbb{R}^n$ tale che $\underline{x} \in \mathbb{E}_n \mapsto [\underline{x}]_{\mathcal{B}}$, dove con $[\underline{x}]_{\mathcal{B}}$ indichiamo il vettore delle coordinate del vettore $\underline{x}$ rispetto alla base $\mathcal{B}$ nello spazio $\mathbb{E}_n$. \\
Per comodità, noi vorremmo che questa base fosse anche ortonormale per semplificare la trattazione (l'esistenza è garantita, ovviamente, dal teorema di Lagrange). 
\begin{definition}[base ortonormale]
	sia $\mathcal{B} = \{ \underline{v}_1, \ldots \underline{v}_n \}$ una base di uno spazio euclideo, diciamo che essa è una base ortonormale se
	$$
	\innerprod {\underline{v}_i} {\underline{v}_j} = \delta_{ij} = \begin{cases} 1 & i = j \\ 0 & i \neq j \end{cases}	
	$$
\end{definition}
\begin{remark}
	Se con la norma indotta dal prodotto scalare lo spazio è completo, allora $\mathbb{E}_n$ è uno spazio di Hilbert.
\end{remark}
\noindent Indichiamo adesso fissata la base ortonormale $(\underline{e}_1, \ldots \underline{e}_n)$. Osserviamo che, dati $\underline{v}, \underline{w}$, allora
$$
\innerprod {\underline{v}}{\underline{w}} = \innerprod{\sum_{j=1}^n x_j \underline{v}_j}{\sum_{i=1}^n y_i \underline{v}_i} = \sum_{j=1}^n \sum_{i=1}^n x_j y_i \innerprod{\underline{v}_j}{\underline{v}_i} = \sum_{j=1}^n \sum_{i=1}^n x_i y_i \delta_{ij} = \sum_{i=1}^n x_i y_i
$$
\begin{definition}[norma]
	Sia $V$ uno spazio vettoriale reale (o complesso). Si definisce norma un'applicazione $| \cdot |: V \to \mathbb{R}$ che verifica le seguenti condizioni:
	\begin{enumerate}[label=\protect\circled{\arabic*}]
		\item $|\underline{v}| \geq 0 \, \, \forall \underline{v} \in V$
		\item $| \underline{v} | = 0 \iff \underline{v} = \underline{0}$
		\item $| \lambda \underline{v} | = | \lambda | | \underline{v} | \, \, \forall \lambda \in \mathbb{R}, \forall \underline{v} \in V$
		\item $| \underline{v} + \underline{w} | \leq | \underline{v} | + | \underline{w} | \, \, \forall \underline{v}, \underline{w} \in V$
	\end{enumerate}
	(analogamente per uno spazio definito su $\mathbb{C}$ sostituendo $\mathbb{C}$ a $\mathbb{R}$)
\end{definition}
\noindent \textbf{Notazione}: la norma di un vettore verrà indicata all'interno di questo documento, per comodità, sia con $|\cdot|$ e sia con $||\cdot||$
\begin{remark}
Osserviamo che se $(V, \varphi)$ è uno spazio euclideo allora $| \cdot | = \sqrt{\varphi(\cdot, \cdot)}$ è una norma: il prodotto scalare \emph{induce} una norma su $V$. Mostreremo più avanti il punto $\circled{4}$ (ovvero la proprietà \emph{meno} banale fra quelle) usando la norma indotta dal prodotto scalare
\end{remark}
\noindent Sapendo che $\mathbb{E}_n \simeq \mathbb{R}^n$(quando quest'ultimo è munito di un prodotto scalare definito positivo naturalmente) possiamo effettuare tutte le dimostrazioni in $\mathbb{R}^n$ e queste saranno naturalmente valide in tutti gli spazi euclidei di dimensione finita $n$. 
\begin{theorem} Sia $(V, \varphi)$ uno spazio euclideo. Allora $$\forall \underline{v}, \underline{w} \in V, \innerprod{\underline{v}}{\underline{w}} = |\underline{v}| \, | \underline{w} | \cos{\hat{\theta}}$$
dove $\hat{\theta}$ è l'angolo convesso fra i due vettori $\underline{v}$ e $\underline{w}$
\end{theorem}
\begin{proof}
consideriamo due vettori $\underline{v} \in \mathbb{E}_2$ con $| \underline{v} | = | \underline{w} | = 1$. Prendiamo per semplicità $\underline{v} = \underline{e}_1$ e si osserva che
$$
\innerprod{\underline{e}_1}{\underline{w}} = \cos{\hat{\theta}}
$$
Questo segue banalmente dall'interpretazione geometrica del prodotto scalare canonico. \\
Per estendere la validità di questo risultato a tutti i $\underline{v} \neq \underline{e}_1$ si osserva che $\exists R \in \text{SO}(\mathbb{E}_2): R\underline{e}_1 = \underline{v}$ e dunque, considerando $R^{-1}(\underline{w})$ (l'esistenza di un'inversa è garantita dal fatto che $R \in \text{SO}(2)$) e sappiamo che:
$$
\innerprod{\underline{v}}{\text{Id}(\underline{w})} = \innerprod{R \underline{e}_1}{(R \circ R^{-1})\underline{w}} \stackrel{R\text{ è isometria}}{=} \innerprod{\underline{e}_1}{R^{-1}(\underline{w})} =  \cos{\hat{\theta}}
$$
A questo punto, dati due vettori qualunque $\underline{v}$ e $\underline{w}$ non di norma unitaria, possiamo utilizzare il ragionamento procedente osservando che:
$$
\innerprod{\frac{\underline{v}}{|\underline{v}|}} {\frac{\underline{w}}{|\underline{w}|}} = \cos{\hat{\theta}}
$$
ma allora, si osserva che:
$$
\frac{1}{|\underline{v}|} \cdot \frac{1}{|\underline{w}|} \innerprod{\underline{v}} {\underline{w}} = \cos{\hat{\theta}} \implies \innerprod{\underline{v}}{ \underline{w}} = |\underline{v}| |\underline{w}| \cos{\hat{\theta}}
$$
dunque la tesi (in $\mathbb{E}_2$). Per generalizzare questo concetto a qualunque spazio, noi sappiamo che possiamo considerare il piano $\pi = \text{Span}(\underline{v}, \underline{w})$ e considerare il loro angolo $\hat{\theta}$ convesso giacente in questo piano.
\end{proof}
\begin{prop}[Disuguaglianza di Cauchy-Schwarz]
$$\forall \underline{v}, \underline{w} \in \mathbb{E}_n, \, | \innerprod{\underline{v}}{\underline{w}} | \leq |\underline{v}| \, |\underline{w}|$$
\label{prop:dis_cs}
\end{prop}
\begin{proof}
Consideriamo $\lambda \in \mathbb{R}$ e sappiamo che, per coercività del prodotto scalare, che:
$$
\forall \underline{v}, \underline{w} \in \mathbb{E}_n, \, \innerprod {\underline{v} + \lambda \underline{w}} {\underline{v} + \lambda \underline{w}} > 0
$$
ma per bilinearità del prodotto scalare abbiamo che
$$
\innerprod {\underline{v} + \lambda \underline{w}} {\underline{v} + \lambda \underline{w}} = \innerprod{\underline{v}}{\underline{v}} + 2\lambda \innerprod{\underline{v}} {\underline{w}} + \lambda^2 \innerprod{\underline{w}}{\underline{w}}> 0
$$
dunque l'equazione in $\lambda$
$$
\innerprod{\underline{v}}{\underline{v}} + 2\lambda \innerprod{\underline{v}} {\underline{w}} + \lambda^2 \innerprod{\underline{w}}{\underline{w}} = 0
$$
non deve avere soluzione, il che implica che
$$
\Delta = 4\innerprod{\underline{v}}{\underline{w}}^2 - 4 \innerprod{\underline{v}}{\underline{v}} \innerprod{\underline{w}}{\underline{w}} < 0 \implies \innerprod{\underline{v}}{\underline{w}}^2 < \innerprod{\underline{v}}{\underline{v}} \innerprod{\underline{w}}{\underline{w}} \implies |\innerprod{\underline{v}}{\underline{w}}| < |\underline{v}| \, |\underline{w}|
$$
\end{proof}
\begin{prop}[Disuguaglianza triangolare] $\forall \underline{v}, \underline{w} \in \mathbb{E}_n, $
\begin{enumerate}[label=\protect\circled{\arabic*}]
	\item $|\underline{v} + \underline{w}| \leq |\underline{v}| + |\underline{w}|$
	\item $| \, |\underline{v}| - |\underline{w}| \, | \leq |\underline{v} - \underline{w}|$
\end{enumerate}
\label{prop:dis_triang}
\end{prop}
\begin{proof}
osserviamo che
\begin{align*}
|\underline{v} + \underline{w}|^2 = \innerprod{\underline{v} + \underline{w}}{\underline{v} + \underline{w}} = \innerprod{\underline{v}}{\underline{v}} + 2\innerprod{\underline{v}}{\underline{w}} + \innerprod{\underline{w}}{\underline{w}} = |\underline{v}|^2 + 2 \innerprod{\underline{v}}{\underline{w}} + |\underline{w}|^2
\end{align*}
Osserviamo che possiamo utilizzare la disuguaglianza di Cauchy-Schwarz per minorare l'ultima espressione ottenuta, da cui si deduce che
\begin{align*}
|\underline{v} + \underline{w}|^2 \leq |\underline{v}|^2 + 2|\underline{v}| \, |\underline{w}| + |\underline{w}|^2
\end{align*}
in cui riconosciamo che $|\underline{v}|^2 + 2 |\underline{v}| \, |\underline{w}| + |\underline{w}|^2 = (|\underline{v}| + |\underline{w}|)^2$, dunque
$$
|\underline{v} + \underline{w}|^2 \leq (|\underline{v}| + |\underline{w}|)^2 \implies |\underline{v} + \underline{w}| \leq |\underline{v}| + |\underline{w}|
$$
e si ottiene la tesi del punto $\circled{1}$. \\
Il punto $\circled{2}$ si ottiene come corollario del primo punto osservando che
$$
|\underline{v}| = |\underline{v} + \underline{w} - \underline{w}| \leq |\underline{v} - \underline{w}| + |\underline{w}| \implies |\underline{v}| - |\underline{w}| \leq |\underline{v} - \underline{w}| 
$$
ma ragionando in maniera identica sul vettore $\underline{w}$ si osserva che
$$
|\underline{w}| = |\underline{w} - \underline{v} + \underline{v}| \leq |\underline{w} - \underline{v}| + |\underline{v}| \implies |\underline{w}| - |\underline{v}| \leq |\underline{w} - \underline{v}|= |\underline{v} - \underline{w}|
$$
dunque possiamo concludere che
$$
||\underline{v}| - |\underline{w}|| \leq |\underline{v} - \underline{w}|
$$
ottenendo la tesi
\end{proof}
\begin{remark}
Come avevo detto in una osservazione, ogni prodotto scalare definito positivo induce sempre una norma. La dimostrazione qua sopra non fa uso di nessuna proprietà specifiche del prodotto scalare canonico di $\mathbb{R}^n$, dunque può essere usata per ogni norma indotta dal prodotto scalare definito positivo di qualunque spazio euclideo
\end{remark}
\section{Il prodotto vettoriale}
\noindent In $\mathbb{R}^3$ (ma in generale in qualunque spazio euclideo di dimensione $3$) è anche possibile definire l'operazione di prodotto vettoriale, molto utile per trattare (come vedremo più avanti) l'orientazione delle superfici. \\
Fissando una base ortonormale $\{\underline{e_1}, \underline{e_2}, \underline{e_3} \}$ di $\mathbb{E}_3$, definendo questa operazione $\times: V \times V \to V$ assegnando i prodotti elementari secondo l'invarianza per permutazioni cicliche, ponendo che
$$
\underline{e_1} \times \underline{e_2} = \underline{e_3}
$$
e, per invarianza per permutazioni cicliche, dovremo avere che
\begin{align*}
&\underline{e_3} \times  \underline{e_1} = \underline{e_2} \\
&\underline{e_2} \times \underline{e_3} = \underline{e_1}
\end{align*}
Le permutazioni non cicliche invece fanno variare il segno dunque avremo, per il vettore $\underline{e_1}$
\begin{align}
	\begin{cases}
		\underline{e_1} \times \underline{e_2} = \underline{e_3} \\
		\underline{e_1} \times \underline{e_3} = - \underline{e_2} \\
		\underline{e_2} \times \underline{e_3} = \underline{e_1}
	\end{cases}
\end{align}
Le regole che abbiamo visto possono essere anche facilmente ottenute tramite la cosiddetta \emph{regola della mano destra}: indicando la direzione del primo vettore con il pollice (in questo caso $\underline{e_1}$) e con l'indice il secondo vettore (in questo caso $\underline{e_2}$), ottenendo sul pollice la direzione del terzo vettore. \\
Vogliamo inoltre che questo prodotto vettoriale sia \emph{bilineare}. \\
Dati adesso due vettori $\underline{v}$ e $\underline{w} \in E_3$ abbiamo che $\underline{v}, \underline{w} \in \text{Span}(\underline{e_1}, \underline{e_2}, \underline{e_3})$ dunque avremo che:
\begin{align*}
&\underline{v} = x_1 \underline{e_1} + x_2 \underline{e_2} + x_3 \underline{e_3} &
&\underline{w} = y_1 \underline{e_1} + y_2 \underline{e_2} + y_3 \underline{e_3}
\end{align*}
e studiamo quali saranno le coordinate del prodotto vettoriale fra questi due:
\begin{align*}
&\underline{v} \times \underline{w} =  (x_1 \underline{e_1} + x_2 \underline{e_2} + x_3 \underline{e_3}) \times (y_1 \underline{e_1} + y_2 \underline{e_2} + y_3 \underline{e_3}) = \\
	&= (x_1 y_2 - x_2 y_1)( \underline{e_1} \times \underline{e_2}) + (x_1 y_3 - x_3 y_1) (\underline{e_1} \times \underline{e_3}) + (x_2 y_3 - x_3 y_2) (\underline{e_2} \times \underline{e_3})
\end{align*}
dunque definiamo una matrice $C$ di questa forma:
\begin{align}
C = \begin{pmatrix}
	x_1 & y_1 \\
	x_2 & y_2 \\
	x_3 & y_3 
\end{pmatrix}
\end{align}
e ne consideriamo i minori:
\begin{align}
M_{ij}(C) = \text{det}\begin{pmatrix}
	x_i & y_i \\
	x_j & y_j
\end{pmatrix} = x_i y_j - x_j y_i
\end{align}
dunque otteniamo una nuova formula:
$$
\underline{v} \times \underline{w} = M_{12}(C)(\underline{e_1} \times \underline{e_2}) + M_{23}(C) (\underline{e_2} \times \underline{e_3}) + M_{13}(C) (\underline{e_1} \times \underline{e_3}) = M_{12}(C)\underline{e_3} + M_{23}(C)\underline{e_1} - M_{13}(C) \underline{e_2}
$$
usando le proprietà del determinante, sappiamo che $M_{13}(C) = -M_{31}(C)$ dunque
$$
\underline{v} \times \underline{w} = M_{12}(C) \underline{e_3} + M_{23}(C) \underline{e_1} + M_{31} \underline{e_2}
$$
abbiamo dunque dimostrato la seguente proposizione
\begin{prop} dati i vettori $\underline{v}, \underline{w} \in \mathbb{E}_3$ allora $\exists x_1, \ldots, x_3, y_1, \ldots y_3 \in \mathbb{R}$ tali che $\underline{v} = x_1 \underline{e_1} + x_2 \underline{e_2} + x_2 \underline{e_3}$ e $\underline{w} = y_1 \underline{e_1} + y_2 \underline{e_2} + y_3 \underline{e_3}$. Posta la matrice $C$ tale che
$$
C = \begin{pmatrix}
	x_1 & y_1 \\
	x_2 & y_2 \\
	x_3 & y_3
\end{pmatrix}
$$
allora
	\begin{equation}
\underline{v} \times \underline{w} = M_{23}(C)\underline{e_1} + M_{31}(C)\underline{e_2} + M_{12}(C)\underline{e_3}
\end{equation}
\end{prop}
\begin{remark}
si noti come i numeri siano tutti disposti secondo permutazioni cicliche: questo può essere un buon trucco per ricordarsela.
\end{remark}
\begin{cor}[matrice del prodotto vettoriale] siano $\underline{v}, \underline{w} \in \mathbb{E}_3$ e siano $x_1, \ldots x_3$ e $y_1, \ldots y_3$ le rispettive coordinate rispetto alla base ortonormale $\mathcal{B} = \{ \underline{e_1}, \underline{e_2}, \underline{e_3}\}$ di $\mathbb{E}_3$. Allora
\begin{equation}
\underline{v} \times \underline{w} = \text{det}\begin{pmatrix}
	x_1 & y_1 & \underline{e_1} \\
	x_2 & y_2 & \underline{e_2} \\
	x_3 & y_3 & \underline{e_3}
\end{pmatrix}
\end{equation}
\label{cor:pr_vett_det}
\end{cor}
\begin{proof}
si osservi che, procedendo con uno sviluppo di Laplace lungo la terza colonna abbiamo che:
$$
\text{det}\begin{pmatrix}
x_1 & y_1 & \underline{e_1} \\
	x_2 & y_2 & \underline{e_2} \\
	x_3 & y_3 & \underline{e_3}
\end{pmatrix} = \underline{e_1} M_{23}(C) - \underline{e_2}M_{13}(C) + \underline{e_3} M_{12}(C) = M_{23}(C) \underline{e_1} + M_{31}(C) \underline{e_2} + M_{12}(C) \underline{e_3}
$$
\end{proof}
\begin{prop}[matrice del prodotto misto]
siano dati i vettori $\underline{v}=x_1 \underline{e_1} + x_2 \underline{e_2} + x_3 \underline{e_3}$ e $\underline{w} = y_1 \underline{e_1} + y_2 \underline{e_2} + y_3 \underline{e_3}$ e $\underline{z} = \xi_1 \underline{e_1} + \xi_2 \underline{e_2} + \xi_3 \underline{z_3}$. Allora
\begin{equation}
\innerprod{\underline{v} \times \underline{w}}{\underline{z}} = \text{det}\begin{pmatrix}
	x_1 & y_1 & \xi_1 \\
	x_2 & y_2 & \xi_2 \\
	x_3 & y_3 & \xi_3
\end{pmatrix}
\end{equation}
\label{prop:prod_mist}
\end{prop}
\begin{proof}
	sappiamo che $\underline{v} \times \underline{w} = M_{23}(C)\underline{e_1} + M_{31}(C) \underline{e_2} + M_{12}(C) \underline{e_3}$ dunque $\innerprod{\underline{v} \times \underline{w}}{\underline{z}} = M_{23}(C)\xi_1 + M_{31}(C)\xi_2 + M_{12}(C)\xi_3$. D'altra parte definendo la matrice 
	$$M=\begin{pmatrix}
		x_1 & y_1 & \xi_1 \\
		x_2 & y_2 & \xi_2 \\
		x_3 & y_3 & \xi_3
	\end{pmatrix} 
$$
allora si osserva che
$$
\innerprod{\underline{v} \times \underline{w}}{\underline{z}} = \text{cof}_{13}(M)\xi_1 + \text{cof}_{23}(M)\xi_2 + \text{cof}_{33}(M) \xi_3 = \text{det}(M)
$$
dunque la tesi.
\end{proof}
\begin{cor}[$\underline{v} \times \underline{w} \perp \text{Span}(\underline{v}, \underline{w})$]
Dati due vettori $\underline{v}, \underline{w} \in \mathbb{E}_3$ allora
$$
\underline{v} \times \underline{w} \, \perp \text{Span}(\underline{v}, \underline{w})
$$
\end{cor}
\begin{proof}
dalla proposizione \ref{prop:prod_mist} sappiamo che
$$
\innerprod{\underline{v} \times \underline{w}}{\underline{v}} = \text{det}\begin{pmatrix}
	x_1 & y_1 & x_1 \\
	x_2 & y_2 & x_2 \\
	x_3 & y_3 & x_3
\end{pmatrix} = 0
$$
per proprietà del determinante. Similmente per il vettore $\underline{w}$. Dunque abbiamo che $\underline{v} \times \underline{w} \perp \underline{v}$ e $\underline{v} \times \underline{w} \perp \underline{w}$ che implica che $\underline{v} \times \underline{w}$ è perpendicolare a qualunque combinazione lineare dei vettori $\underline{v}$ e $\underline{w}$, quindi $\underline{v} \times \underline{w} \perp \text{Span}(\underline{v}, \underline{w})$ ovvero la tesi.
\end{proof}
\begin{cor}[norma del prodotto vettoriale]
Siano $\underline{v}, \underline{w} \in \mathbb{E}_3$ con $\underline{v} = x_1 \underline{e_1} + x_2 \underline{e_2} + x_3 \underline{e_3}$ e $\underline{w} = y_1 \underline{e_1} + y_2 \underline{e_2} + y_3 \underline{e_3}$. Allora, ponendo
$$
C = \begin{pmatrix}
	x_1 & y_1 \\
	x_2 & y_2 \\
	x_3 & y_3 
\end{pmatrix}
$$
allora
$$
|\underline{v} \times \underline{w}| = \sqrt{M_{23}(C)^2 + M_{12}(C)^2 + M_{31}(C)^2} 
$$
\label{cor:norm_cross_prod}
\end{cor}
\begin{proof}
Osserviamo, per la proposizione \ref{prop:prod_mist}, che
\begin{align*}
&\innerprod{\underline{v} \times \underline{w}}{\underline{v} \times \underline{w}} = \text{det}\begin{pmatrix}
\underline{v} & \underline{w} & \underline{v} \times \underline{w}
\end{pmatrix} = M_{23}(C) (\underline{v} \times \underline{w})_1 + M_{31}(C) (\underline{v} \times \underline{w})_2 + M_{12}(C) (\underline{v} \times \underline{w})_3 = \\
&= M_{23}(C)^2 + M_{31}(C)^2 + M_{12}(C)^2
\end{align*}
\noindent dunque $|\underline{v} \times \underline{w}| = \sqrt{M_{23}(C)^2 + M_{31}(C)^2 + M_{12}(C)^2}$
\end{proof}
\begin{prop}
Sia $R \in \text{SO}(3)$ e siano dati $\underline{v}, \underline{w} \in \mathbb{E}_3$. Allora
\begin{equation}
R(\underline{v} \times \underline{w}) = R\underline{v} \times R\underline{w}
\end{equation}
\end{prop}
\begin{proof}
Sia $\underline{u} \in \mathbb{E}_3$ e consideriamo il seguente prodotto scalare:
\begin{align*}
\innerprod{R\underline{v} \times R\underline{w}}{R\underline{u}} &= \text{det}\begin{pmatrix}
R\underline{v} & R\underline{w} & R\underline{u}
\end{pmatrix} \\
&= \text{det}\left(R \begin{pmatrix}
\underline{v} & \underline{w} & \underline{u} 
\end{pmatrix}\right)
\end{align*}
Usando il teorema di Binet abbiamo che
$$
\innerprod{R\underline{v} \times R\underline{w}}{R\underline{u}} = \text{det}\left(R \begin{pmatrix}
\underline{v} & \underline{w} & \underline{u} 
\end{pmatrix}\right) = \text{det}(R)\text{det}\begin{pmatrix}
\underline{v} & \underline{w} & \underline{u} 
\end{pmatrix} = \text{det}\begin{pmatrix}
	\underline{v} & \underline{w} & \underline{u}
\end{pmatrix}
$$
dove abbiamo usato l'ipotesi che $R \in \text{SO}(3)$ dunque $\text{det}(R) = 1$ e siccome sappiamo che $R$ è un'isometria allora
$$
\innerprod{R\underline{v} \times R\underline{w}}{R\underline{u}} = \text{det}\begin{pmatrix} \underline{v} & \underline{w} & \underline{u} \end{pmatrix} = \innerprod{\underline{v} \times \underline{w}}{\underline{u}} = \innerprod{R(\underline{v} \times \underline{w})}{R\underline{u}}
$$
ma allora $\innerprod{R\underline{v} \times R\underline{w}}{R\underline{u}} = \innerprod{R(\underline{v} \times \underline{w})}{R\underline{u}} \implies \innerprod{R(\underline{v} \times \underline{w}) - (R\underline{v} \times R\underline{w})}{R\underline{u}} = 0$. Ma allora, per la coercività del prodotto scalare, abbiamo necessariamente che
$$
R(\underline{v} \times \underline{w}) = R\underline{v} \times R\underline{w}
$$
\end{proof}
\begin{prop}
Se $\underline{v}, \underline{w} \in \mathbb{R}^3, \underline{v} \perp \underline{w}$ e $|\underline{v}| = |\underline{w}| = 1$ allora
$$
|\underline{v} \times \underline{w}| = 1
$$
\label{prop:norm_unit_cross_prod}
\end{prop}
\begin{proof}
Dati $\underline{v}$ e $\underline{w}$ con $\underline{v} \perp \underline{w}$, allora $\{\underline{v}, \underline{w}, \underline{v} \times \underline{w} \}$ è una base. Siccome appartengono a $\mathbb{R}^3$ e sono perpendicolari, allora sappiamo che $\exists R \in \text{SO}(3): R\underline{v} = \underline{e_1}$, $R\underline{w} = \underline{e_2}$ e $R(\underline{v} \times \underline{w}) = \underline{e_3}$ dunque
$$
|\underline{v} \times \underline{w}| = 1 = |R\underline{e_1} \times R\underline{e_2}|= |R(\underline{e_1} \times \underline{e_2})| = |\underline{v} \times \underline{w}| \implies |\underline{v} \times \underline{w}| = 1
$$
\end{proof}
\begin{cor}[modulo del prodotto vettoriale di vettori perpendicolari]
siano $\underline{v}, \underline{w} \in \mathbb{E}_3$ e $\underline{v} \perp \underline{w}$ allora:
$$
	|\underline{v} \times \underline{w}| = |\underline{v}| \, |\underline{w}|
$$
\end{cor}
\begin{proof}
siccome $\underline{v} \perp \underline{w}$ allora $\frac{\underline{v}}{|\underline{v}|} \perp \frac{\underline{w}}{|\underline{w}|}$. Ma allora possiamo applicare a questi due vettori la proposizione precedente, dunque:
$$
\Big|\frac{\underline{v}}{|\underline{v}|} \times \frac{\underline{w}}{|\underline{w}|} \Big| = 1 = \frac{1}{|\underline{v}| \, |\underline{w}|} |\underline{v} \times \underline{w}| \implies |\underline{v} \times \underline{w}| = |\underline{v}| \, |\underline{w}|
$$
\end{proof}
\begin{prop}[area del parallelogramma]
Siano $\underline{v}, \underline{w} \in \mathbb{E}_3$. Allora $$|\underline{v} \times \underline{w}| = |\underline{v}| \, |\underline{w}| \sin{\hat{\theta}}$$
dove $\hat{\theta}$ è l'angolo convesso fra i due vettori
\end{prop}
\begin{proof}
possiamo "ortogonalizzare" il vettore $\underline{v}$ usando il procedimento di Grand-Schmit, dunque
\begin{align*}|\underline{v} \times \underline{w}|^2 = \Bigg| &\left( \underline{v} - \frac{\innerprod{\underline{v}}{\underline{w}}}{|\underline{w}|^2} \underline{w} \right) \times \underline{w} \Bigg|^2 = \Bigg| \underline{v} - \frac{\innerprod{\underline{v}}{\underline{w}}}{|\underline{w}|^2} \underline{w} \Bigg|^2 |\underline{w}|^2 = \innerprod{\underline{v} - \frac{\innerprod{\underline{v}}{\underline{w}}}{|\underline{w}|^2} \underline{w}}{\underline{v} - \frac{\innerprod{\underline{v}}{\underline{w}}}{|\underline{w}|^2} \underline{w}} |\underline{w}|^2 = \\
&= |\underline{w}|^2 \left( \innerprod{\underline{v}}{\underline{v}} - 2 \innerprod{\underline{v}}{\frac{\innerprod{\underline{v}}{\underline{w}}}{|\underline{w}|^2} \underline{w}} + \frac{(\innerprod{\underline{v}}{\underline{w}})^2}{|\underline{w}|^2} \innerprod{\underline{w}}{\underline{w}} \right) = \\
&= |\underline{w}|^2 \left( \innerprod{\underline{v}}{\underline{v}} - 2 \frac{\innerprod{\underline{v}}{\underline{w}}}{|\underline{w}|^2} \innerprod{\underline{v}}{\underline{w}} + \frac{(\innerprod{\underline{v}}{\underline{w}})^2}{|\underline{w}|^2} \innerprod{\underline{w}}{\underline{w}} \right) = \\
&= |\underline{w}|^2 \left( \innerprod{\underline{v}}{\underline{v}} - \frac{(\innerprod{\underline{v}}{\underline{w}})^2}{|\underline{w}|^2} \right)
\end{align*}
Sappiamo adesso che $\innerprod{\underline{v}}{\underline{w}} = |\underline{v}| \, |\underline{w}|\cos{\hat{\theta}}$, dunque
$$
|\underline{w}|^2 \left(|\underline{v}|^2 - |\underline{v}^2|\cos^2{\hat{\theta}} \right) = |\underline{v}|^2 |\underline{w}|^2 (1-\cos^2{\hat{\theta}}) = |\underline{v}|^2 |\underline{w}|^2 \sin^2{\hat{\theta}}
$$
ma allora se ne conclude che
$$
|\underline{v} \times \underline{w}|^2 = |\underline{v}|^2 |\underline{w}|^2 \sin^2{\hat{\theta}} \implies |\underline{v} \times \underline{w}| = |\underline{v}| \, |\underline{w}|\sin{\hat{\theta}}
$$
e questo conclude la dimostrazione.
\end{proof}
\begin{remark}
Unendo quest'ultima proposizione con il corollario \ref{cor:norm_cross_prod} possiamo facilmente vedere che
$$
|\underline{v} \times \underline{w}| = |\underline{v}| \, |\underline{w}| \sin{\hat{\theta}} = \sqrt{M_{23}(C)^2 + M_{12}(C)^2 + M_{31}(C)^2}
$$
\end{remark}
\begin{definition}[base positivamente orientata]
diremo che tre vettori $\underline{v}, \underline{w}, \underline{z} \in \mathbb{E}_3$ linearmente indipendenti sono una base positivamente orientata se
$$
\text{det}\begin{pmatrix}
	\underline{v} & \underline{w} & \underline{z}
\end{pmatrix} > 0
$$
\end{definition}
\begin{remark}
La base canonica di $\mathbb{R}^3$ è positivamente orientata, siccome
$$
\text{det}\begin{pmatrix}
	1 & 0 & 0 \\
	0 & 1 & 0 \\
	0 & 0 & 1
\end{pmatrix} = \text{det}(I) = 1
$$
\end{remark}
\begin{prop}
	Se $\underline{v}, \underline{w} \in \mathbb{R}^3$ sono linearmente indipendenti, allora l'insieme $\mathcal{B} = \{\underline{v}, \underline{w}, \underline{v} \times \underline{w} \}$ è una base positivamente orientata
\label{prop:base_pos_orient}
\end{prop}
\begin{proof}
banalmente
$$
\text{det}\begin{pmatrix}
	\underline{v} & \underline{w} & \underline{v} \times \underline{w} \end{pmatrix} = \innerprod{\underline{v} \times \underline{w}}{\underline{v} \times \underline{w}} = |\underline{v} \times \underline{w}|^2 >  0
$$
\end{proof}
\begin{remark}
Una base positivamente orientata si può sempre rappresentare con la regola della mano destra: il vettore $\underline{v}$ è rappresentato dall'indice, il vettore $\underline{w}$ è rappresentato dal dito medio e il pollice invece rappresenta $\underline{v} \times \underline{w}$.
\end{remark}
\noindent Infine, per concludere per adesso la lista dei teoremi sul prodotto vettoriale, abbiamo che:
\begin{prop}
Una base ortonormale $\{ \underline{v} , \underline{w} , \underline{v} \times \underline{w} \}$ è positivamente orientata se e solo se $\underline{z} = \underline{v} \times \underline{w}$
\end{prop}
\begin{proof} \hfill \\
$\boxed{\Leftarrow} \,$: già mostrato per la proposizione \ref{prop:base_pos_orient} \\
$\boxed{\Rightarrow} \,$: supponiamo di avere la seguente base ortonormale positivamente orientata $\{\underline{i}, \underline{j}, \underline{k} \}$, allora abbiamo che questa segue la regola della mano destra, dunque $\underline{i} \times \underline{j} = \alpha \underline{k}$ con $k \in \mathbb{R}$. Ma allora, per la proposizione \ref{prop:norm_unit_cross_prod}, abbiamo che $|\alpha \underline{k}|=1$ (siccome $\underline{v} \perp \underline{w}$) dunque $\underline{i} \times \underline{j} = \underline{k}$. 	
\end{proof}
\section{Topologia di $\mathbb{R}^n$}
Come visto dal corso di Analisi Matematica del primo anno, possiamo pensare $\mathbb{R}^n$ come uno spazio metrico se consideriamo la distanza indotta dal prodotto scalare. Non introdurremo in questa sezione "tutta" la topologia necessaria per l'Analisi siccome sarebbe, secondo la mia opinione, troppo dispersivo siccome rallenterebbe lo studio; dunque mi sono focalizzato nel mostrare una buona parte delle definizioni e nel fornire degli esercizi per impratichirsi. In ogni caso, per coloro che sono interessati
ad una trattazione più specifica nella materia (pur conservando un approccio rivolto verso l'applicazione all'Analisi matematica) consiglio di sfogliare l'eccellente capitolo presente in \cite{rudin} o nel \cite{ambrosio} oppure un buon libro di Topologia, di cui consiglio \cite{manetti} oppure \cite{munkres}. \\
Possiamo dunque andare a definire le \textbf{palle}:
\begin{definition}[palle aperte e chiuse]
Sia $x \in \mathbb{R}^n$ e fissiamo $r > 0$. Allora definiamo la palla aperta di raggio $r$ nella seguente maniera
$$
B(x, r) = \{ z \in \mathbb{R}^n: |z-x| < r \}
$$
e la palla chiusa di raggio $r$ nella seguente maniera
$$
\mathbb{B}(x, r) = \{ z \in \mathbb{R}^n: |z - x| \leq r \}
$$
con centro $x$ e raggio $r$.
\end{definition}
\noindent Ma cosa vuol dire palla aperta? In generale possiamo definire la nozione di insieme aperto nella seguente maniera:
\begin{definition}[insieme aperto]
$A \subset \mathbb{R}^n$ è un insieme aperto se $\forall y \in A \exists r > 0: B(y, r_y) \subset A$
\end{definition}
\begin{exercise}
Mostrare che $B(x, r)$ è un aperto $\forall x \, \forall r$
\end{exercise}
\begin{proof}[Svolgimento]
Sia $x \in \mathbb{R}^n$ e sia fissato $r > 0$. Allora scelto $y \in B(x, r)$ sappiamo che preso $\varepsilon = r - |x - y| > 0$ allora 
possiamo definire la palla $B(y, \varepsilon)$ e prendere $z \in B(y, \varepsilon)$ e osservare che
$$
|z-x| \leq |z-y| + |y-x| < r - |x-y| + |x-y| = r
$$
dunque $|z-x| < r$ il che implica che $B(y, \varepsilon) \subset B(x, r)$
\end{proof}
\begin{exercise}
Siano $\{A_j: j \in J \}$ una famiglia di aperti (non necessariamente numerabile), allora $\bigcup\limits_{j \in J} A_j$ è un insieme aperto.
\label{exercise:union_open_set_is_open}
\end{exercise}
\begin{proof}[Svolgimento]
si osserva che, siccome $A_j$ è un insieme aperto $\forall j$, allora $\forall a \in A_j \exists r > 0: B(a, r) \subset A_j$, ma allora $B(a, r) \subset \bigcup\limits_{j \in J} A_j$, dunque $\forall a \in \bigcup\limits_{j \in J} A_j \exists r > 0: B(a, r) \subset \bigcup\limits_{j \in J} A_j$
\end{proof}
\begin{exercise}
Mostrare che presi $A_1, \ldots A_k \subseteq \mathbb{R}^n$ insieme aperti, allora $\bigcap\limits_i^k A_i$ è un aperto
\label{exercise:intersec_open_set_is_open}
\end{exercise}
\begin{proof}[Svolgimento]
Sia $x \in \bigcap\limits_{i}^k A_i \implies \, \, \forall i \in \{1, \ldots, n \}, x \in A_i$ e, siccome per ipotesi sono aperti, avremo che $\forall i \, \, \exists r_i(x) : B(x, r_i(x)) \subseteq A_i$, dunque preso $r=\min\limits_i{ \{ r_i(x) : i \in \{1, \ldots, k \} \} }$ avremo che $\forall i \in \{1, \ldots, k \}, B(x, r) \subseteq A_i \implies B(x, r) \subseteq \bigcup\limits_i^k A_i$,
dunque $\forall x \in \bigcap\limits_{i}^k A_i \, \, \exists r > 0 : B(x, r) \subseteq \bigcap\limits_i^k A_i$, dunque è aperto per definizione.
\end{proof}
\begin{exercise}
Sia $A=\{(x, y) \in \mathbb{R}^2 : 1 < x^2 + y^2 < 4 \} = B(0, 2) \setminus \mathbb{B}(0,1)$, provare che $A$ è aperto
\end{exercise}
\begin{proof}[Svolgimento]
	Fissiamo $x \in A$ e prendiamo come $r=\min{ \{ 4-|x|, |x|-1 \} }$ avremo che
	$$
	|y| \leq |y-x+x| \leq |y-x| + |x| < |x| + r \stackrel{r \leq 4 - |x|^2}{\leq} |x| + 4 - |x| = 4
	$$
	A questo punto osserviamo che:
	$$
	|y| \geq -|x-y| + |x| \geq |x| - r > 1
	$$
	dunque in conclusione abbiamo che $\forall y \in B(x, r), 1 < |y| < 4$. Avremo quindi che $A$ è aperto.
\end{proof}
\begin{exercise}
Provare che $A = \{(x, y, z) : -1 < x < 2 \} = (-1, 2) \times \mathbb{R}^2$ ed è un insieme aperto in $\mathbb{R}^3$
\end{exercise}
\begin{proof}[Svolgimento] \hspace{1cm} \\
	Mostriamo che $A \subseteq (-1, 2) \times \mathbb{R}^2$: osserviamo che, per definizione di $A$, $-1<x<2$ e, non essendoci altri vincoli, $y$, $z \in \mathbb{R}^2$; pertanto $A \subseteq (-1, 2) \times \mathbb{R}^2$. \\
	Mostriamo che $(-1, 2) \times \mathbb{R}^2 \subseteq A$: osserviamo che, per definizione di prodotto cartesiano, preso $(x, y, z) \in \mathbb{R}^3$ allora $x \in (-1, 2) \implies -1 < x < 2$ dunque $(-1, 2) \times \mathbb{R}^2 \subseteq A$. \\
	Dimostriamo che è aperto: preso $x'=(x, y, z) \in A$ allora possiamo considerare $r = \min{ \{x+1, 2-x \} }$ avremo che preso $y \in B(x', r)$ allora, posto $y=(y_x, y_y, y_z)$ e $x'=(x, x'_y, x'_z)$, avremo che
	$$
	|x - y_x| \leq |x'-y|^2 < r \implies |y_x - x| < r \implies y_x \in (x - r, x + r) 
	$$
	ma $x + r \leq x + 2 - x = 2 \implies x + r \leq 2$ e $x - r \geq x - x + 1 = 1$, dunque $y_x \in (-1, 2)$.
\end{proof}
\begin{exercise}
	Provare che $\mathbb{R}^n$ è aperto
\end{exercise}
\begin{proof}[Svolgimento]
	Sia $x \in \mathbb{R}^n$, allora preso $r > 0$ e la palla $B(x, r)$ avremo che, per definizione, $\forall y \in B(x, r), y \in \mathbb{R}^n \implies B(x, r) \subseteq \mathbb{R}^n$.
\end{proof}

Un concetto chiave sono i cosidetti \emph{punti di frontiera}, definiti come
\begin{definition}
	Sia $x \in \mathbb{R}^n$ e sia $A \subseteq \mathbb{R}^n$, diremo che $x$ è un punto di frontiera per $A$ se $\forall r > 0, B(x, r) \cap A \neq \emptyset$ e $B(x, r) \cap A^c \neq \emptyset$. \\
	Definiamo
	$$
	\text{Fr}(A)=\{x \in \mathbb{R}^n : x \text{di frontiera} \}
	$$
\end{definition}
\begin{exercise}
	Provare che $\text{Fr} \, B(x, r) = \{y \in \mathbb{R}^n : |x-y| = r \}$ e $\text{Fr} \, \mathbb{B}(x, r) = \{y \in \mathbb{R}^n : |x-y| = r \}$
\end{exercise}
\begin{proof}[Svolgimento]
	Mostriamo che $\text{Fr} \, B(x, r) = \{y \in \mathbb{R}^n : |x-y| = r \}$: %sia $y \in \text{Fr} \, B(x, r)$ allora avremo che $\forall r > 0: B(y, r) \cap B(x, r) \neq \emptyset$ e $B(y, r) \cap (B(x, r))^c \neq \emptyset$. \\
	%Possiamo dunque una successione $r_k = \frac{1}{k}$ e dovremo avere che $B(y, r_k) \cap B(x, r) \neq \emptyset$ e $B(y, r_k) \cap (B(x, r))^c \neq \emptyset$ per quanto detto prima, 
	possiamo osservare che $\forall z \in \{y \in \mathbb{R}^n : |x-y| = r \}, B(z, \varepsilon) \cap B(x, r) \neq \emptyset$, infatti possiamo considerare innanzitutto il segmento che congiunge il punto $y$ e $x$
	$$
	y = z + t(x-z), t \in [0, 1] \implies |x-y| = |(1-t)(x-z)| \implies |x-y| = |1-t|r
	$$
	la condizione che $|x-y|<r$ si ottiene ponendo che $|x-y| < r$, ovvero $|1-t| r < r \implies |1-t| < 1$. Considerando a questo punto il segmento che congiunge $z$ con $y$, avremo che
	$$
	|z-y| < \varepsilon \implies |z-y| = |z-(z+t(x-z))| = |t(x-z)| = t|x-z| = tr < \varepsilon \implies t < \frac{\varepsilon}{r} 
	$$
	pertanto, rimettendo insieme le condizioni trovate, dobbiamo risolvere il seguente sistema di disequazioni:
	\begin{equation*}
		\begin{cases}
			&|1-t| < 1 \implies 0 < t < 2 \\
			&t < \frac{\varepsilon}{r}
		\end{cases}
	\end{equation*}
	che è sempre vero $\forall t \in (0, \frac{\varepsilon}{r})$. Possiamo fare lo stesso per mostrare che $B(z, \varepsilon) \cap (B(x, r))^c$, dunque $\{y \in \mathbb{R}^n: |x-y| = r \} \subseteq \text{Fr} \, B(x, r)$. Mostriamo adesso che vale il viceversa mostrando che 
	se $z \not\in \{y \in \mathbb{R}^n : |x-y| = r \} \implies x \not\in \text{Fr} \, B(x, r)$ (il che equivale a mostrare che $\text{Fr} \, B(x,r) \subseteq \{y \in \mathbb{R}^n : |x-y| = r \}$), siccome sono possibili due casi:
	\begin{itemize}
		\item $|x-z| < r \implies z \in B(x, r)$;
		\item $|x-z| > r \implies z \in (B(x, r))^c$
	\end{itemize}
	e non è possibile che $z \in \text{Fr} \, B(x, r)$ siccome se per assurdo $\forall r' > 0, B(z, r') \cap B(x, r) \neq \emptyset \wedge B(z, r') \cap (B(x, r))^c$ sappiamo che, dall'apertura di $B(x, r)$, $\exists r'' > 0: B(z, r'') \subseteq A$ ma allora
	$$
	(B(x, r))^c \cap B(z, r'') \subseteq B(x, r)
	$$
	il che è un assurdo siccome $(B(x,r))^c \cap B(x, r) = \emptyset$. \\
	Per la sfera chiusa è analoga.
\end{proof}
\begin{definition}[punti di aderenza]
	Sia $A \neq \emptyset$ e $A \subseteq \mathbb{R}^n$, diremo che $x \in \mathbb{R}^n$ è un punto di aderenza o di chiusura di $A$ se $\forall r > 0, B(x, r) \cap A \neq \emptyset$. \\
	Inoltre indicheremo con $\bar{A}$ l'insieme dei punti di aderenza di $A$.
\end{definition}
\begin{remark}
	Naturalmente $A \subseteq \bar{A}$, siccome $\forall r > 0: B(x, r) \cap A \supseteq \{ x \} \neq \emptyset$
\end{remark}
\begin{definition}[insieme chiuso]
	Un insieme $F$ è chiuso se contiene tutti i suoi punti di aderenza, ovvero $F = \bar{F}$.
\end{definition}
\begin{exercise}
	Provare che $\text{Fr} \, A = \bar{A} \cap \bar{A^c}$
	\label{exercise:frontiera_intersec}
\end{exercise}
\begin{proof}[Svolgimento]
	Sia $x \in \text{Fr} \, A \implies \forall r > 0, B(x, r) \cap A \neq \emptyset \wedge B(x, r) \cap A^c \neq \emptyset$, ma allora $\forall r > 0, B(x, r) \cap A \neq \emptyset \implies x$ è un punto di aderenza di $A$, dunque $x \in \bar{A}$, e $\forall r > 0, B(x, r) \cap A^c \neq \emptyset \implies x$ è un punto
	di aderenza di $A^c$, pertanto $x \in \bar{A} \cap \bar{A^c}$. Sia $x \in \bar{A} \cap \bar{A^c}$, allora, per definizione, avremo che $\forall r > 0: B(x, r) \cap A \neq \emptyset$ e $\forall r > 0: B(x, r) \cap A^c \neq \emptyset$, dunque $x \in \text{Fr} \, A$.
\end{proof}
\begin{exercise}
	$A \subseteq \mathbb{R}^n$ è chiuso $\iff \text{Fr} \, A \subseteq A$
\end{exercise}
\begin{proof}[Svolgimento] \hspace{1cm} \\
	$\boxed{\Rightarrow}$: osserviamo che se $A$ è chiuso, allora $A = \bar{A}$, pertanto siccome $\text{Fr} \, A \subseteq \bar{A} \implies \text{Fr} \, A \subseteq A$. Si ha necessariamente che $\text{Fr} \, A \subseteq \bar{A}$ siccome $\text{Fr} \, A = \bar{A} \cap \bar{A^c}$. \\
	$\boxed{\Leftarrow}$: se $\text{Fr} \, A \subseteq A$ allora preso $x \in \bar{A}$ avremo due possibilità:
	\begin{itemize}
		\item $x \in A$;
		\item $x \in \text{Fr} \, A \subseteq A \implies x \in A$
	\end{itemize}
	dunque $\bar{A} \subseteq A \implies A = \bar{A}$.
\end{proof}
\begin{exercise}
	Provare che $\bar{A} = A \cup \text{Fr} \, A$
\end{exercise}
\begin{proof}[Svolgimento] \hspace{1cm} \\
	Mostriamo che vale $\bar{A} \supseteq A \cup \text{Fr} \, A$: è banale vedere che $A \subseteq \bar{A}$ e, in virtù di quanto visto in un esercizio precedente, siccome $\text{Fr} \, A = \bar{A} \cap \bar{A^c} \implies \text{Fr} \, A \subseteq \bar{A} \implies A \cup \text{Fr} \, A \subseteq \bar{A}$. \\
	L'inclusione opposta si ottiene prendendo $x \in \bar{A}$ e ragionando come sopra.
\end{proof}
\begin{definition}[punti interni]
	Sia $A \subseteq \mathbb{R}^n$, diremo che $x$ è un punto interno di $A$ se $\exists r > 0: B(x, r) \subseteq A$
\end{definition}
\begin{remark}
$A$ aperto $\iff A = \text{Int} \, A$
\end{remark}
\begin{prop}
$F$ è chiuso $\iff F^c$ è aperto
\label{prop:set_closed_iff_compl_open}
\end{prop}
\begin{proof} \hspace{1cm} \\
	$\boxed{\Rightarrow}$: se $F$ è chiuso e $z \in F^c \implies z \in \text{Int} \, (F^c)$. Se per assurdo $z \not\in \text{Int} \, (F^c)$ allora $\forall r > 0: B(z, r) \not\subseteq F^c \implies B(z, r) \cap F \neq \emptyset \implies z \in \bar{F} = F \implies z \in F$, che contraddice l'ipotesi iniziale. \\
	$\boxed{\Leftarrow}$: se $F^c$ è aperto, allora preso $z \in \bar{F}$ se, per assurdo, avessimo che $z \not\in F \implies z \in F^c$ allora $\exists \varepsilon > 0: B(z, \varepsilon) \subseteq F^c$ (per l'ipotesi di apertura), pertanto $B(z, \varepsilon) \cap F = \emptyset \implies z \not\in \bar{F}$, il che contraddice l'ipotesi iniziale. 
\end{proof}
